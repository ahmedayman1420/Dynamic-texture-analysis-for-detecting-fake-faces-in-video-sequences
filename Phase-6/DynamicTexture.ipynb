{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G6gvCgiQwuNe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.2.2\n"
          ]
        }
      ],
      "source": [
        "# ========== ----- ========== Import Libraries ========== ----- ========== #\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import svm\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "from joblib import dump, load\n",
        "from collections import Counter\n",
        "import pickle\n",
        "from LDP import LDP_TOP\n",
        "from LDP import train\n",
        "import sklearn\n",
        "print(sklearn.__version__)\n",
        "\n",
        "\n",
        "# ========== ----- ========== End ========== ----- ========== #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== ----- ========== Dynamic Texture Class ========== ----- ========== #\n",
        "\n",
        "class dynamicTexture:\n",
        "\n",
        "    # ========== ----- ========== Class Initialization ========== ----- ========== #\n",
        "\n",
        "    def __init__(self, dataset_version):\n",
        "        \"\"\"\n",
        "        Initializes an instance of the dynamicTexture class.\n",
        "\n",
        "        :param dataset_version: The version of the dataset ('cf23' or 'cf40').\n",
        "        \"\"\"\n",
        "        assert dataset_version == 'cf23' or dataset_version == 'cf40', \"dataset_version is not equal to 'cf23' or 'cf40'\"\n",
        "        self.dataset_version = dataset_version\n",
        "\n",
        "# ========== ----- ========== Feature Extraction Train ========== ----- ========== #\n",
        "\n",
        "    def feature_extraction_train(self, train_frame_folders, features_file_name, labels_file_name):\n",
        "\n",
        "        # Face detector\n",
        "        detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "        list_of_train_LDP = []\n",
        "        list_of_train_labels = []\n",
        "\n",
        "        for train_frame_folder in train_frame_folders:\n",
        "            list_of_train_data = [f for f in os.listdir(\n",
        "                train_frame_folder) if f.endswith('.mp4')]\n",
        "            print(train_frame_folder, 'videos: ', len(list_of_train_data))\n",
        "            for vid in list_of_train_data:\n",
        "                frames = []\n",
        "                # Video capturing constructor.\n",
        "                cap = cv2.VideoCapture(os.path.join(train_frame_folder, vid))\n",
        "\n",
        "                # (CAP_PROP_FPS) Returns frame rate of the video (#frames / second).\n",
        "                frameRate = cap.get(5)\n",
        "                while cap.isOpened():  # Returns true if video capturing has been initialized already.\n",
        "\n",
        "                    # (CAP_PROP_POS_MSEC) Current position of the video file in milliseconds or video capture timestamp.\n",
        "                    frameId = cap.get(1)\n",
        "                    ret, frame = cap.read()  # The methods/functions combine VideoCapture::grab and VideoCapture::retrieve in one call. This is the most convenient method for reading video files or capturing data from decode and return the just grabbed frame. If no frames has been grabbed (camera has been disconnected, or there are no more frames in video file), the methods return false and the functions return NULL pointer.\n",
        "                    if ret != True:\n",
        "                        break\n",
        "\n",
        "                    face_rects, scores, idx = detector.run(frame, 0)\n",
        "                    for i, d in enumerate(face_rects):\n",
        "                        x1 = d.left()\n",
        "                        y1 = d.top()\n",
        "                        x2 = d.right()\n",
        "                        y2 = d.bottom()\n",
        "\n",
        "                        crop_img = frame[y1:y2, x1:x2]\n",
        "                        if crop_img is not None and crop_img.size > 0:\n",
        "                            crop_img = cv2.resize(crop_img, (128, 128))\n",
        "                            gray_scale_img = cv2.cvtColor(\n",
        "                                crop_img, cv2.COLOR_BGR2GRAY)\n",
        "                            frames.append(gray_scale_img)\n",
        "\n",
        "                    # d = 3 seconds.\n",
        "                    if frameId % ((int(frameRate)+1)*3) == 0:\n",
        "                        if (len(frames) > 0):\n",
        "                            frames_ldp = LDP_TOP(\n",
        "                                np.array(frames).astype(np.float64))\n",
        "                            list_of_train_LDP.append(frames_ldp)\n",
        "\n",
        "                            if ((self.dataset_version == 'cf23' and train_frame_folder == 'datasets/cf23/faceforensics_dataset_train/original_sequences')\n",
        "                                    or (self.dataset_version == 'cf40' and train_frame_folder == 'datasets/cf40/faceforensics_dataset_train/original_sequences')):\n",
        "                                list_of_train_labels.append(0)\n",
        "                            else:\n",
        "                                list_of_train_labels.append(1)\n",
        "                        frames = []\n",
        "\n",
        "        with open(features_file_name, 'wb') as f:\n",
        "            pickle.dump(list_of_train_LDP, f)\n",
        "\n",
        "        with open(labels_file_name, 'wb') as f:\n",
        "            pickle.dump(list_of_train_labels, f)\n",
        "\n",
        "# ========== ----- ========== Feature Extraction Test ========== ----- ========== #\n",
        "\n",
        "    def feature_extraction_test(self, test_frame_folders, features_file_name):\n",
        "\n",
        "        # Face detector\n",
        "        detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "        list_of_test_LDP = []\n",
        "\n",
        "        index = 0\n",
        "        for test_frame_folder in test_frame_folders:\n",
        "            list_of_test_data = [f for f in os.listdir(\n",
        "                test_frame_folder) if f.endswith('.mp4')]\n",
        "\n",
        "            print(test_frame_folder, 'videos: ', len(list_of_test_data))\n",
        "            for vid in list_of_test_data:\n",
        "                list_of_test_LDP.append([])\n",
        "                frames = []\n",
        "                # Video capturing constructor.\n",
        "                cap = cv2.VideoCapture(os.path.join(test_frame_folder, vid))\n",
        "\n",
        "                # (CAP_PROP_FPS) Returns frame rate of the video (#frames / second).\n",
        "                frameRate = cap.get(5)\n",
        "                while cap.isOpened():  # Returns true if video capturing has been initialized already.\n",
        "                    # (CAP_PROP_POS_MSEC) Current position of the video file in milliseconds or video capture timestamp.\n",
        "                    frameId = cap.get(1)\n",
        "                    ret, frame = cap.read()  # The methods/functions combine VideoCapture::grab and VideoCapture::retrieve in one call. This is the most convenient method for reading video files or capturing data from decode and return the just grabbed frame. If no frames has been grabbed (camera has been disconnected, or there are no more frames in video file), the methods return false and the functions return NULL pointer.\n",
        "                    if ret != True:\n",
        "                        break\n",
        "\n",
        "                    face_rects, scores, idx = detector.run(frame, 0)\n",
        "                    for i, d in enumerate(face_rects):\n",
        "                        x1 = d.left()\n",
        "                        y1 = d.top()\n",
        "                        x2 = d.right()\n",
        "                        y2 = d.bottom()\n",
        "\n",
        "                        crop_img = frame[y1:y2, x1:x2]\n",
        "                        if crop_img is not None and crop_img.size > 0:\n",
        "                            crop_img = cv2.resize(crop_img, (128, 128))\n",
        "                            gray_scale_img = cv2.cvtColor(\n",
        "                                crop_img, cv2.COLOR_BGR2GRAY)\n",
        "                            frames.append(gray_scale_img)\n",
        "\n",
        "                    # d = 3 seconds.\n",
        "                    if frameId % ((int(frameRate)+1)*3) == 0:\n",
        "                        if (len(frames) > 0):\n",
        "                            frames_ldp = LDP_TOP(\n",
        "                                np.array(frames).astype(np.float64))\n",
        "                            list_of_test_LDP[index].append(frames_ldp)\n",
        "\n",
        "                        frames = []\n",
        "                index += 1\n",
        "\n",
        "        with open(features_file_name, 'wb') as f:\n",
        "            pickle.dump(list_of_test_LDP, f)\n",
        "\n",
        "# ========== ----- ========== Loading Features ========== ----- ========== #\n",
        "\n",
        "    def loading_features(self, features_file_name, labels_file_name):\n",
        "\n",
        "        # Loading the list_of_train_LDP\n",
        "        with open(features_file_name, 'rb') as f:\n",
        "            list_of_train_LDP = pickle.load(f)\n",
        "\n",
        "        # Loading the list_of_train_labels\n",
        "        with open(labels_file_name, 'rb') as f:\n",
        "            list_of_train_labels = pickle.load(f)\n",
        "\n",
        "        return list_of_train_LDP, list_of_train_labels\n",
        "\n",
        "# ========== ----- ========== Train ========== ----- ========== #\n",
        "\n",
        "    def train(self, list_of_train_LDP, list_of_train_labels, rbf_model_name, linear_model_name):\n",
        "\n",
        "        model = svm.SVC(kernel='rbf')\n",
        "        model.fit(list_of_train_LDP, list_of_train_labels)\n",
        "        # Save the trained model to a file\n",
        "        dump(model, rbf_model_name)\n",
        "        # ================================================ #\n",
        "        model = svm.SVC(kernel='linear')\n",
        "        model.fit(list_of_train_LDP, list_of_train_labels)\n",
        "        # Save the trained model to a file\n",
        "        dump(model, linear_model_name)\n",
        "\n",
        "# ========== ----- ========== Single Technique Test ========== ----- ========== #\n",
        "\n",
        "    def single_technique_test(self, list_of_test_LDP, rbf_model, linear_model):\n",
        "        rbf_predictions = []\n",
        "        linear_predictions = []\n",
        "\n",
        "        for vid in list_of_test_LDP:\n",
        "            y_pred = rbf_model.predict(vid)\n",
        "            if y_pred.tolist().count(1) >= y_pred.tolist().count(0):\n",
        "                rbf_predictions.append(1)\n",
        "            else:\n",
        "                rbf_predictions.append(0)\n",
        "            # ============================================= #\n",
        "            y_pred = linear_model.predict(vid)\n",
        "            if y_pred.tolist().count(1) >= y_pred.tolist().count(0):\n",
        "                linear_predictions.append(1)\n",
        "            else:\n",
        "                linear_predictions.append(0)\n",
        "\n",
        "            labels = [0]*140 + [1]*140\n",
        "\n",
        "        # calculate the accuracy score and print it out\n",
        "        accuracy = accuracy_score(labels, rbf_predictions)\n",
        "        print(\"RBF Accuracy:\", accuracy)\n",
        "        # calculate the confusion matrix and print it out\n",
        "        cm = confusion_matrix(labels, rbf_predictions)\n",
        "        print(\"RBF Confusion matrix:\\n\", cm, \"\\n\")\n",
        "        # calculate the accuracy score and print it out\n",
        "        accuracy = accuracy_score(labels, linear_predictions)\n",
        "        print(\"Linear Accuracy:\", accuracy)\n",
        "        # calculate the confusion matrix and print it out\n",
        "        cm = confusion_matrix(labels, linear_predictions)\n",
        "        print(\"Linear Confusion matrix:\\n\", cm)\n",
        "\n",
        "# ========== ----- ========== Multiple Technique Test With Binary Models ========== ----- ========== #\n",
        "\n",
        "    def multiple_technique_test_with_binary_models(self, list_of_test_LDP, linear_models):\n",
        "        linear_predictions = []\n",
        "        multi_linear_predictions = []\n",
        "        for vid in list_of_test_LDP:\n",
        "            y_pred = []\n",
        "            y_score = []\n",
        "\n",
        "            df_pred = linear_models[0].predict(vid)\n",
        "            if df_pred.tolist().count(1) >= df_pred.tolist().count(0):\n",
        "                y_score.append(df_pred.tolist().count(1))\n",
        "                y_pred.append(1)\n",
        "            else:\n",
        "                y_score.append(0)\n",
        "                y_pred.append(0)\n",
        "            # ============================================= #\n",
        "            if self.dataset_version == 'cf23':\n",
        "                f2f_pred = linear_models[1].predict(vid)\n",
        "                if f2f_pred.tolist().count(1) >= f2f_pred.tolist().count(0):\n",
        "                    y_score.append(f2f_pred.tolist().count(1))\n",
        "                    y_pred.append(1)\n",
        "                else:\n",
        "                    y_score.append(0)\n",
        "                    y_pred.append(0)\n",
        "            # ============================================= #\n",
        "            fsw_pred = linear_models[2].predict(vid)\n",
        "            if fsw_pred.tolist().count(1) >= fsw_pred.tolist().count(0):\n",
        "                y_score.append(fsw_pred.tolist().count(1))\n",
        "                y_pred.append(1)\n",
        "            else:\n",
        "                y_score.append(0)\n",
        "                y_pred.append(0)\n",
        "            # ============================================= #\n",
        "            if self.dataset_version == 'cf23':\n",
        "                nt_pred = linear_models[3].predict(vid)\n",
        "                if nt_pred.tolist().count(1) >= nt_pred.tolist().count(0):\n",
        "                    y_score.append(nt_pred.tolist().count(1))\n",
        "                    y_pred.append(1)\n",
        "                else:\n",
        "                    y_score.append(0)\n",
        "                    y_pred.append(0)\n",
        "            # ============================================= #\n",
        "            if y_pred.count(1) > 0:\n",
        "                linear_predictions.append(1)\n",
        "                multi_linear_predictions.append(y_score.index(max(y_score))+1)\n",
        "            else:\n",
        "                linear_predictions.append(0)\n",
        "                multi_linear_predictions.append(0)\n",
        "\n",
        "        if self.dataset_version == 'cf23':\n",
        "            labels = [0]*140 + [1]*560\n",
        "        elif self.dataset_version == 'cf40':\n",
        "            labels = [0]*140 + [1]*280\n",
        "\n",
        "        # calculate the accuracy score and print it out\n",
        "        accuracy = accuracy_score(labels, linear_predictions)\n",
        "        print(\"Linear Accuracy:\", accuracy)\n",
        "\n",
        "        # calculate the confusion matrix and print it out\n",
        "        cm = confusion_matrix(labels, linear_predictions)\n",
        "        print(\"Linear Confusion matrix:\\n\", cm)\n",
        "        # ============================================= #\n",
        "        if self.dataset_version == 'cf23':\n",
        "            labels = [0]*140 + [1]*140 + [2]*140 + [3]*140 + [4]*140\n",
        "        elif self.dataset_version == 'cf40':\n",
        "            labels = [0]*140 + [1]*140 + [3]*140\n",
        "\n",
        "        # calculate the accuracy score and print it out\n",
        "        accuracy = accuracy_score(labels, multi_linear_predictions)\n",
        "        print(\"Multi-Linear Accuracy:\", accuracy)\n",
        "\n",
        "        # calculate the confusion matrix and print it out\n",
        "        cm = confusion_matrix(labels, multi_linear_predictions)\n",
        "        print(\"Multi-Linear Confusion matrix:\\n\", cm)\n",
        "\n",
        "# ========== ----- ========== Multiple Technique Test With Multi Models ========== ----- ========== #\n",
        "\n",
        "    def multiple_technique_test_with_multi_model(self, list_of_test_LDP, multi_model):\n",
        "        linear_predictions = []\n",
        "        multi_predictions = []\n",
        "\n",
        "        for vid in list_of_test_LDP:\n",
        "            y_pred = multi_model.predict(vid)\n",
        "            # print(y_pred)\n",
        "            if y_pred.tolist().count(0) > (len(y_pred) // 2):\n",
        "                linear_predictions.append(0)\n",
        "            else:\n",
        "                linear_predictions.append(1)\n",
        "\n",
        "            count_dict = Counter(y_pred)\n",
        "            most_frequent_number = max(count_dict, key=count_dict.get)\n",
        "            multi_predictions.append(most_frequent_number)\n",
        "\n",
        "        labels = [0]*140 + [1]*560\n",
        "        # calculate the accuracy score and print it out\n",
        "        accuracy = accuracy_score(labels, linear_predictions)\n",
        "        print(\"Linear Accuracy:\", accuracy)\n",
        "        # calculate the confusion matrix and print it out\n",
        "        cm = confusion_matrix(labels, linear_predictions)\n",
        "        print(\"Linear Confusion matrix:\\n\", cm)\n",
        "        # ============================================= #\n",
        "        labels = [0]*140 + [1]*140 + [2]*140 + [3]*140 + [4]*140\n",
        "        # calculate the accuracy score and print it out\n",
        "        accuracy = accuracy_score(labels, multi_predictions)\n",
        "        print(\"Multi-Linear Accuracy:\", accuracy)\n",
        "        # calculate the confusion matrix and print it out\n",
        "        cm = confusion_matrix(labels, multi_predictions)\n",
        "        print(\"Multi-Linear Confusion matrix:\\n\", cm)\n",
        "\n",
        "# ========== ----- ========== End ========== ----- ========== #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DS_LDPcode.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
