{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G6gvCgiQwuNe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.2.2\n"
          ]
        }
      ],
      "source": [
        "# ========== ----- ========== Import Libraries ========== ----- ========== #\n",
        "\n",
        "from joblib import dump, load\n",
        "from collections import Counter\n",
        "import pickle\n",
        "from LDP import LDP_TOP\n",
        "from LDP import train\n",
        "import sklearn\n",
        "print(sklearn.__version__)\n",
        "\n",
        "import numpy as np\n",
        "import dlib\n",
        "import cv2\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ========== ----- ========== End ========== ----- ========== #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== ----- ========== Reading Training Data ========== ----- ========== #\n",
        "\n",
        "# Reading Training Data\n",
        "train_frame_folders = [\n",
        "    'faceforensics_dataset_train/original_sequences',\n",
        "    # 'faceforensics_dataset_train/manipulated_sequences/Deepfakes',\n",
        "    # 'faceforensics_dataset_train/manipulated_sequences/Face2Face',\n",
        "    # 'faceforensics_dataset_train/manipulated_sequences/FaceSwap',\n",
        "    # 'faceforensics_dataset_train/manipulated_sequences/NeuralTextures'\n",
        "]\n",
        "\n",
        "# ========== ----- ========== End ========== ----- ========== #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== ----- ========== Feature Extraction ========== ----- ========== #\n",
        "\n",
        "# Face detector\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "list_of_train_LDP = []\n",
        "list_of_train_labels = []\n",
        "\n",
        "for train_frame_folder in train_frame_folders:\n",
        "    list_of_train_data = [f for f in os.listdir(\n",
        "        train_frame_folder) if f.endswith('.mp4')]\n",
        "    print(train_frame_folder, 'videos: ', len(list_of_train_data))\n",
        "    for vid in list_of_train_data:\n",
        "        frames = []\n",
        "        # Video capturing constructor.\n",
        "        cap = cv2.VideoCapture(os.path.join(train_frame_folder, vid))\n",
        "\n",
        "        # (CAP_PROP_FPS) Returns frame rate of the video (#frames / second).\n",
        "        frameRate = cap.get(5)\n",
        "        while cap.isOpened():  # Returns true if video capturing has been initialized already.\n",
        "\n",
        "            # (CAP_PROP_POS_MSEC) Current position of the video file in milliseconds or video capture timestamp.\n",
        "            frameId = cap.get(1)\n",
        "            ret, frame = cap.read()  # The methods/functions combine VideoCapture::grab and VideoCapture::retrieve in one call. This is the most convenient method for reading video files or capturing data from decode and return the just grabbed frame. If no frames has been grabbed (camera has been disconnected, or there are no more frames in video file), the methods return false and the functions return NULL pointer.\n",
        "            if ret != True:\n",
        "                break\n",
        "\n",
        "            face_rects, scores, idx = detector.run(frame, 0)\n",
        "            for i, d in enumerate(face_rects):\n",
        "                x1 = d.left()\n",
        "                y1 = d.top()\n",
        "                x2 = d.right()\n",
        "                y2 = d.bottom()\n",
        "\n",
        "                crop_img = frame[y1:y2, x1:x2]\n",
        "                if crop_img is not None and crop_img.size > 0:\n",
        "                    crop_img = cv2.resize(crop_img, (128, 128))\n",
        "                    gray_scale_img = cv2.cvtColor(crop_img, cv2.COLOR_BGR2GRAY)\n",
        "                    frames.append(gray_scale_img)\n",
        "\n",
        "            # d = 3 seconds.\n",
        "            if frameId % ((int(frameRate)+1)*3) == 0:\n",
        "                if (len(frames) > 0):\n",
        "                    frames_ldp = LDP_TOP(np.array(frames).astype(np.float64))\n",
        "                    list_of_train_LDP.append(frames_ldp)\n",
        "                    if train_frame_folder == 'faceforensics_dataset_train/original_sequences':\n",
        "                        list_of_train_labels.append(0)\n",
        "                    else:\n",
        "                        list_of_train_labels.append(1)\n",
        "                frames = []\n",
        "\n",
        "# ========== ----- ========== End ========== ----- ========== #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== ----- ========== Load Features ========== ----- ========== #\n",
        "\n",
        "# Loading the list_of_train_LDP\n",
        "with open('features/list_of_train_LDP.pkl', 'rb') as f:\n",
        "    list_of_train_LDP = pickle.load(f)\n",
        "\n",
        "# Loading the list_of_train_labels\n",
        "with open('features/list_of_train_labels.pkl', 'rb') as f:\n",
        "    list_of_train_labels = pickle.load(f)\n",
        "\n",
        "# ========== ----- ========== End ========== ----- ========== #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0: 5666\n",
            "1: 5666\n",
            "2: 5669\n",
            "3: 4570\n",
            "4: 4570\n",
            "0: 5666\n",
            "4: 4570\n",
            "0: 5666\n",
            "1: 4570\n"
          ]
        }
      ],
      "source": [
        "# ========== ----- ========== Prepare Features ========== ----- ========== #\n",
        "\n",
        "counts = Counter(list_of_train_labels)\n",
        "\n",
        "for value, count in counts.items():\n",
        "    print(f\"{value}: {count}\")\n",
        "\n",
        "# NeuralTextures\n",
        "list_of_train_labels = list_of_train_labels[:5666] + \\\n",
        "    list_of_train_labels[21571:26141]\n",
        "\n",
        "list_of_train_LDP = list_of_train_LDP[:5666] + \\\n",
        "    list_of_train_LDP[21571:26141]\n",
        "\n",
        "counts = Counter(list_of_train_labels)\n",
        "\n",
        "for value, count in counts.items():\n",
        "    print(f\"{value}: {count}\")\n",
        "\n",
        "list_of_train_labels = [0] * 5666 + [1] * 4570\n",
        "\n",
        "counts = Counter(list_of_train_labels)\n",
        "\n",
        "for value, count in counts.items():\n",
        "    print(f\"{value}: {count}\")\n",
        "    \n",
        "# ========== ----- ========== End ========== ----- ========== #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Linear-NeuralTextures.joblib']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ========== ----- ========== Training SVM ========== ----- ========== #\n",
        "\n",
        "model = svm.SVC(kernel='rbf')\n",
        "model.fit(list_of_train_LDP, list_of_train_labels)\n",
        "# Save the trained model to a file\n",
        "dump(model, 'RBF-NeuralTextures.joblib')\n",
        "\n",
        "# ================================================ #\n",
        "\n",
        "model = svm.SVC(kernel='linear')\n",
        "model.fit(list_of_train_LDP, list_of_train_labels)\n",
        "# Save the trained model to a file\n",
        "dump(model, 'Linear-NeuralTextures.joblib')\n",
        "\n",
        "# ========== ----- ========== End ========== ----- ========== #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DS_LDPcode.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
